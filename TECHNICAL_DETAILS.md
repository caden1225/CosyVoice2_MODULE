# CosyVoice2 æµå¼åˆ†å—æ¨ç† - æŠ€æœ¯å®ç°ç»†èŠ‚

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ `inference_sft_chunked` æ–¹æ³•çš„æŠ€æœ¯å®ç°åŸç†ã€ä½¿ç”¨çš„æ¨¡å‹ç»“æ„ã€ä»¥åŠè¯·æ±‚åŒºåˆ†æœºåˆ¶ã€‚

---

## ğŸ“‹ ç›®å½•

1. [Chunkæ¨¡å¼çš„å®ç°åŸç†](#chunkæ¨¡å¼çš„å®ç°åŸç†)
   - [åˆ©ç”¨çš„ç°æœ‰æ¨¡å‹ç»“æ„](#åˆ©ç”¨çš„ç°æœ‰æ¨¡å‹ç»“æ„)
   - [å®ç°æµç¨‹](#å®ç°æµç¨‹)
   - [ä¸åŸæœ‰æ–¹æ³•çš„åŒºåˆ«](#ä¸åŸæœ‰inference_sftçš„åŒºåˆ«)
2. [è¯·æ±‚åŒºåˆ†æœºåˆ¶](#è¯·æ±‚åŒºåˆ†æœºåˆ¶)
   - [å¦‚ä½•åŒºåˆ†åŒä¸€è¯·æ±‚çš„ä¸åŒchunk](#å¦‚ä½•åŒºåˆ†åŒä¸€è¯·æ±‚çš„ä¸åŒchunk)
   - [å¦‚ä½•åŒºåˆ†ä¸åŒçš„è¯·æ±‚](#å¦‚ä½•åŒºåˆ†ä¸åŒçš„è¯·æ±‚)
   - [ç¼“å­˜ç”Ÿå‘½å‘¨æœŸ](#ç¼“å­˜ç”Ÿå‘½å‘¨æœŸ)
   - [å¹¶å‘è¯·æ±‚æ”¯æŒ](#å¹¶å‘è¯·æ±‚æ”¯æŒ)

---

## Chunkæ¨¡å¼çš„å®ç°åŸç†

### åˆ©ç”¨çš„ç°æœ‰æ¨¡å‹ç»“æ„

`inference_sft_chunked` æ–¹æ³•é‡‡ç”¨äº†**é›¶ä¿®æ”¹æ ¸å¿ƒæ¨¡å‹ä»£ç **çš„è®¾è®¡ç†å¿µï¼Œå®Œå…¨åŸºäºCosyVoice2å·²æœ‰çš„åŠŸèƒ½å®ç°ã€‚è¿™ä¸ªå®ç°åˆ©ç”¨äº†ä¸‰ä¸ªå…³é”®çš„ç°æœ‰æœºåˆ¶ï¼š

#### 1. æ ¸å¿ƒæœºåˆ¶: LLMçš„`inference_bistream`æ–¹æ³•

**æ–‡ä»¶ä½ç½®**: `cosyvoice/llm/llm.py:505-602`

**åŸæœ‰åŠŸèƒ½**:
- LLMæ¨¡å—å·²ç»æ”¯æŒæ¥å—**Generatorç±»å‹**çš„textè¾“å…¥
- è¿™ä¸ªæ–¹æ³•æœ¬æ¥æ˜¯ä¸ºæµå¼æ–‡æœ¬è¾“å…¥è®¾è®¡çš„
- æ”¯æŒé€æ­¥æ¶ˆè´¹è¾“å…¥tokenï¼Œè€Œä¸éœ€è¦ä¸€æ¬¡æ€§æ¥æ”¶æ‰€æœ‰token

**æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨**:
- å°†å¤šä¸ªæ–‡æœ¬chunkçš„tokenè½¬æ¢ä¸ºä¸€ä¸ªè¿ç»­çš„Generatoræµ
- LLMæ¨¡å—ä¼šè‡ªåŠ¨æ¶ˆè´¹è¿™ä¸ªGeneratorï¼Œé€tokenå¤„ç†
- LLMå†…éƒ¨ç»´æŠ¤KV-cacheï¼Œä¿è¯è¯­ä¹‰è¿è´¯æ€§

**ä»£ç ç¤ºä¾‹**:
```python
# llm.pyä¸­çš„å…³é”®æ–¹æ³•ç­¾å
def inference_bistream(self, text: Generator, prompt_text, llm_prompt_speech_token, ...):
    """
    æ¥å—Generatorç±»å‹çš„textè¾“å…¥ï¼Œç”¨äºæµå¼æ¨ç†

    Args:
        text: Generator[torch.Tensor] - é€æ­¥yield tokençš„ç”Ÿæˆå™¨
        ...
    """
    for text_token in text:  # é€æ­¥æ¶ˆè´¹Generator
        # ç»´æŠ¤KV-cacheï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­
        # ç”Ÿæˆspeech token
        yield speech_token
```

**å…³é”®ç‚¹**:
- å½“ä¼ å…¥çš„`text`å‚æ•°ç±»å‹æ˜¯`Generator`æ—¶ï¼ŒLLMä¼šè‡ªåŠ¨è°ƒç”¨`inference_bistream`
- è¿™ä¸ªæ–¹æ³•ä½¿ç”¨`mix_ratio=[5,15]`æ¥äº¤é”™ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³token
- KV-cacheè·¨æ‰€æœ‰chunkå…±äº«ï¼Œç¡®ä¿è¯­ä¹‰è¿è´¯

---

#### 2. çŠ¶æ€ç®¡ç†: UUIDä¼šè¯æœºåˆ¶

**æ–‡ä»¶ä½ç½®**: `cosyvoice/cli/model.py`

**åŸæœ‰åŠŸèƒ½**:
- æ¨¡å‹ä½¿ç”¨UUIDæ¥æ ‡è¯†ä¸€æ¬¡æ¨ç†ä¼šè¯
- æ¯ä¸ªUUIDå¯¹åº”ä¸€ç»„ç‹¬ç«‹çš„ç¼“å­˜ï¼ˆLLMã€Flowã€HiFiGANï¼‰
- åŒä¸€ä¸ªUUIDä¸‹çš„æ‰€æœ‰æ“ä½œå…±äº«ç¼“å­˜çŠ¶æ€

**æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨**:
- å•æ¬¡`inference_sft_chunked`è°ƒç”¨ç”Ÿæˆä¸€ä¸ªå”¯ä¸€UUID
- è¿™ä¸ªUUIDåœ¨å¤„ç†æ‰€æœ‰text chunksæ—¶ä¿æŒä¸å˜
- æ‰€æœ‰chunkå…±äº«åŒä¸€ç»„ç¼“å­˜ï¼Œç¡®ä¿è¿è´¯æ€§

**ä»£ç ç¤ºä¾‹**:
```python
# model.pyä¸­çš„UUIDæœºåˆ¶
def tts(self, text, ...):
    uuid = str(uuid4())  # ç”Ÿæˆä¼šè¯ID

    # ä¸ºè¿™ä¸ªä¼šè¯åˆ›å»ºç¼“å­˜
    self.llm_cache[uuid] = {}      # LLMçš„KV-cache
    self.flow_cache[uuid] = {}     # Flowçš„melç¼“å­˜
    self.hift_cache[uuid] = {}     # HiFiGANçš„éŸ³é¢‘ç¼“å­˜

    # æ‰§è¡Œæ¨ç†...

    # æ¨ç†å®Œæˆåæ¸…ç†ç¼“å­˜
    del self.llm_cache[uuid]
    del self.flow_cache[uuid]
    del self.hift_cache[uuid]
```

**ç¼“å­˜å†…å®¹**:
- **LLMç¼“å­˜**: å­˜å‚¨Transformerçš„Key-Value cacheï¼Œä¿æŒè¯­ä¹‰ä¸Šä¸‹æ–‡
- **Flowç¼“å­˜**: å­˜å‚¨melé¢‘è°±çš„è¾¹ç•Œå¸§ï¼Œç”¨äºå¹³æ»‘è¿‡æ¸¡
- **HiFiGANç¼“å­˜**: å­˜å‚¨éŸ³é¢‘æ³¢å½¢çš„è¾¹ç•Œæ ·æœ¬ï¼Œç”¨äºæ·¡å…¥æ·¡å‡º

---

#### 3. æµå¼è¾“å‡º: Flowå’ŒHiFiGANçš„streamingæ”¯æŒ

**ç›¸å…³ä½ç½®**:
- Flow: `cosyvoice/flow/flow.py:161` (`pre_lookahead_len=3`)
- HiFiGANç¼“å­˜: `cosyvoice/cli/model.py:272-277`

**åŸæœ‰åŠŸèƒ½**:
- Flowæ¨¡å‹æ”¯æŒ`streaming=True`å‚æ•°ï¼Œå¯ä»¥é€æ­¥ç”Ÿæˆmelé¢‘è°±
- HiFiGANæ”¯æŒç¼“å­˜æœºåˆ¶ï¼Œå®ç°chunkè¾¹ç•Œçš„å¹³æ»‘è¿‡æ¸¡
- ä½¿ç”¨Hammingçª—è¿›è¡Œæ·¡å…¥æ·¡å‡ºï¼Œé¿å…éŸ³é¢‘æ–­è£‚

**æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨**:
- é€šè¿‡`token_hop_len`å‚æ•°æ§åˆ¶æ¯æ¬¡yieldçš„éŸ³é¢‘é•¿åº¦
- é€šè¿‡`mel_cache_len`å‚æ•°æ§åˆ¶è¾¹ç•Œå¹³æ»‘çš„ç¼“å­˜å¤§å°
- åˆ©ç”¨å·²æœ‰çš„`speech_window`ï¼ˆHammingçª—ï¼‰è¿›è¡Œå¹³æ»‘

**å…³é”®å‚æ•°**:
```python
# model.pyä¸­çš„æµå¼å‚æ•°
self.token_hop_len = 25        # æ¯25ä¸ªtoken yieldä¸€æ¬¡éŸ³é¢‘
self.mel_cache_len = 8         # ç¼“å­˜8å¸§melç”¨äºå¹³æ»‘
self.source_cache_len = 3840   # éŸ³é¢‘æ ·æœ¬ç¼“å­˜ (8 * 480)
self.speech_window = np.hamming(2 * 3840)  # Hammingå¹³æ»‘çª—
```

**Flowçš„lookaheadæœºåˆ¶**:
```python
# flow.py
self.pre_lookahead_len = 3  # éœ€è¦æå‰çœ‹3ä¸ªtoken
```
è¿™æ„å‘³ç€Flowåœ¨ç”Ÿæˆç¬¬Nä¸ªtokençš„éŸ³é¢‘æ—¶ï¼Œéœ€è¦çŸ¥é“ç¬¬N+1, N+2, N+3ä¸ªtokençš„ä¿¡æ¯ï¼Œä»¥ä¿è¯éŸµå¾‹çš„è‡ªç„¶æ€§ã€‚

---

### å®ç°æµç¨‹

ä»¥ä¸‹æ˜¯å®Œæ•´çš„æ•°æ®æµå’Œå¤„ç†æµç¨‹ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”¨æˆ·è°ƒç”¨                                                         â”‚
â”‚ model.inference_sft_chunked(                                    â”‚
â”‚     text_chunks=["ä½ å¥½", "ä¸–ç•Œ", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"],                â”‚
â”‚     spk_id="girl_zh",                                           â”‚
â”‚     stream=True,                                                â”‚
â”‚     token_hop_len=15,                                           â”‚
â”‚     mel_cache_len=6                                             â”‚
â”‚ )                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤1: ç”ŸæˆUUIDä¼šè¯æ ‡è¯†                                          â”‚
â”‚ uuid = "a1b2c3d4-5678-90ef-ghij-klmnopqrstuv"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤2: åˆ›å»ºæ–‡æœ¬tokenç”Ÿæˆå™¨                                       â”‚
â”‚                                                                 â”‚
â”‚ def text_token_generator():                                    â”‚
â”‚     for chunk in ["ä½ å¥½", "ä¸–ç•Œ", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"]:              â”‚
â”‚         # 2.1 æ–‡æœ¬å½’ä¸€åŒ–                                        â”‚
â”‚         normalized = frontend.text_normalize(chunk)            â”‚
â”‚                                                                 â”‚
â”‚         # 2.2 æå–æ–‡æœ¬token                                     â”‚
â”‚         text_token, _ = frontend._extract_text_token(...)      â”‚
â”‚                                                                 â”‚
â”‚         # 2.3 é€token yield (æ— chunkè¾¹ç•Œæ ‡è®°!)                  â”‚
â”‚         for i in range(text_token.shape[1]):                   â”‚
â”‚             yield text_token[:, i:i+1]                         â”‚
â”‚                                                                 â”‚
â”‚ å…³é”®: Generatorå°†æ‰€æœ‰chunkçš„tokenèåˆä¸ºè¿ç»­æµ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤3: å‡†å¤‡æ¨¡å‹è¾“å…¥                                              â”‚
â”‚                                                                 â”‚
â”‚ model_input = {                                                â”‚
â”‚     'text': text_token_generator(),  â† Generatorç±»å‹!          â”‚
â”‚     'llm_embedding': spk_embedding,                            â”‚
â”‚     'flow_embedding': spk_embedding,                           â”‚
â”‚     ...                                                        â”‚
â”‚ }                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤4: LLM.inference_bistream å¤„ç†                               â”‚
â”‚                                                                 â”‚
â”‚ for text_token in text:  â† æ¶ˆè´¹Generator                       â”‚
â”‚     # 4.1 ç»´æŠ¤KV-cache                                         â”‚
â”‚     kv_cache = llm_cache[uuid]                                 â”‚
â”‚                                                                 â”‚
â”‚     # 4.2 Transformer forward                                  â”‚
â”‚     hidden_state = transformer(text_token, kv_cache)           â”‚
â”‚                                                                 â”‚
â”‚     # 4.3 ç”Ÿæˆspeech token                                     â”‚
â”‚     speech_token = predictor(hidden_state)                     â”‚
â”‚                                                                 â”‚
â”‚     # 4.4 yield speech token                                   â”‚
â”‚     yield speech_token                                         â”‚
â”‚                                                                 â”‚
â”‚ å…³é”®: æ‰€æœ‰chunkçš„tokenå…±äº«åŒä¸€ä¸ªKV-cache                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤5: Flowæ¨¡å‹å¤„ç† (streaming=True)                             â”‚
â”‚                                                                 â”‚
â”‚ flow_cache = flow_cache[uuid]  # åŠ è½½melç¼“å­˜                   â”‚
â”‚                                                                 â”‚
â”‚ for i, speech_token in enumerate(speech_tokens):               â”‚
â”‚     # 5.1 æ£€æŸ¥æ˜¯å¦ç§¯ç´¯è¶³å¤Ÿtoken                                 â”‚
â”‚     if len(buffered_tokens) >= token_hop_len:                  â”‚
â”‚                                                                 â”‚
â”‚         # 5.2 ç”Ÿæˆmelé¢‘è°± (å¸¦lookahead)                        â”‚
â”‚         mel = flow.forward(                                    â”‚
â”‚             buffered_tokens,                                   â”‚
â”‚             cache=flow_cache,                                  â”‚
â”‚             lookahead=3  # æå‰çœ‹3ä¸ªtoken                      â”‚
â”‚         )                                                      â”‚
â”‚                                                                 â”‚
â”‚         # 5.3 ä½¿ç”¨mel_cache_lenå¸§è¿›è¡Œè¾¹ç•Œå¹³æ»‘                  â”‚
â”‚         if i > 0:                                              â”‚
â”‚             mel[:, :mel_cache_len] = blend(                    â”‚
â”‚                 flow_cache['last_mel_frames'],                 â”‚
â”‚                 mel[:, :mel_cache_len]                         â”‚
â”‚             )                                                  â”‚
â”‚                                                                 â”‚
â”‚         # 5.4 ä¿å­˜è¾¹ç•Œå¸§ä¾›ä¸‹æ¬¡ä½¿ç”¨                              â”‚
â”‚         flow_cache['last_mel_frames'] = mel[:, -mel_cache_len:]â”‚
â”‚                                                                 â”‚
â”‚         yield mel                                              â”‚
â”‚                                                                 â”‚
â”‚ æ¯token_hop_lenä¸ªtoken yieldä¸€æ¬¡mel                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤6: HiFiGANå£°ç å™¨å¤„ç†                                         â”‚
â”‚                                                                 â”‚
â”‚ hift_cache = hift_cache[uuid]  # åŠ è½½éŸ³é¢‘ç¼“å­˜                  â”‚
â”‚                                                                 â”‚
â”‚ for mel in mel_chunks:                                         â”‚
â”‚     # 6.1 ç”ŸæˆéŸ³é¢‘æ³¢å½¢                                         â”‚
â”‚     audio = hifigan.inference(mel)                             â”‚
â”‚                                                                 â”‚
â”‚     # 6.2 è¾¹ç•Œå¹³æ»‘ (ä½¿ç”¨Hammingçª—)                             â”‚
â”‚     if hift_cache['last_audio'] is not None:                   â”‚
â”‚         # æ·¡å…¥æ·¡å‡ºå¤„ç†                                         â”‚
â”‚         overlap_len = source_cache_len                         â”‚
â”‚         audio[:overlap_len] = (                                â”‚
â”‚             hift_cache['last_audio'][-overlap_len:] *          â”‚
â”‚             speech_window[:overlap_len] +                      â”‚
â”‚             audio[:overlap_len] *                              â”‚
â”‚             speech_window[overlap_len:]                        â”‚
â”‚         )                                                      â”‚
â”‚                                                                 â”‚
â”‚     # 6.3 ä¿å­˜è¾¹ç•ŒéŸ³é¢‘ä¾›ä¸‹æ¬¡ä½¿ç”¨                               â”‚
â”‚     hift_cache['last_audio'] = audio[-source_cache_len:]       â”‚
â”‚                                                                 â”‚
â”‚     yield {'tts_speech': audio}                                â”‚
â”‚                                                                 â”‚
â”‚ ä½¿ç”¨Hammingçª—å®ç°å¹³æ»‘è¿‡æ¸¡                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤7: è¿”å›ç»™ç”¨æˆ·                                                â”‚
â”‚                                                                 â”‚
â”‚ for result in generator:                                       â”‚
â”‚     audio_chunk = result['tts_speech']                         â”‚
â”‚     play_or_stream(audio_chunk)                                â”‚
â”‚                                                                 â”‚
â”‚ ç”¨æˆ·é€ä¸ªæ¥æ”¶éŸ³é¢‘chunkï¼Œå®æ—¶æ’­æ”¾                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤8: æ¸…ç†ç¼“å­˜                                                  â”‚
â”‚                                                                 â”‚
â”‚ del llm_cache[uuid]                                            â”‚
â”‚ del flow_cache[uuid]                                           â”‚
â”‚ del hift_cache[uuid]                                           â”‚
â”‚                                                                 â”‚
â”‚ é‡Šæ”¾å†…å­˜ï¼Œä¼šè¯ç»“æŸ                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ä¸åŸæœ‰`inference_sft`çš„åŒºåˆ«

| æ–¹é¢ | `inference_sft` (ä¼ ç»Ÿæ–¹æ³•) | `inference_sft_chunked` (æ–°æ–¹æ³•) |
|------|---------------------------|--------------------------------|
| **æ–‡æœ¬è¾“å…¥** | å•ä¸ªå­—ç¬¦ä¸² `str` | å¤šä¸ªchunkçš„åˆ—è¡¨æˆ–Generator `List[str]` æˆ– `Generator[str]` |
| **UUIDç”Ÿå‘½å‘¨æœŸ** | æ¯æ¬¡è°ƒç”¨ç”Ÿæˆæ–°UUID<br>â†’ ç‹¬ç«‹ä¼šè¯ | æ•´ä¸ªchunkåºåˆ—å…±äº«ä¸€ä¸ªUUID<br>â†’ è¿ç»­ä¼šè¯ |
| **LLMè¾“å…¥ç±»å‹** | `text: torch.Tensor`<br>â†’ è°ƒç”¨`inference`æ–¹æ³• | `text: Generator`<br>â†’ è°ƒç”¨`inference_bistream`æ–¹æ³• |
| **ä¸Šä¸‹æ–‡è¿ç»­æ€§** | å•æ¬¡ç‹¬ç«‹å¤„ç†<br>æ— è·¨è°ƒç”¨ä¸Šä¸‹æ–‡ | è·¨chunkä¿æŒKV-cache<br>è¯­ä¹‰å®Œå…¨è¿è´¯ |
| **ç¼“å­˜å…±äº«** | ä¸é€‚ç”¨ï¼ˆå•æ¬¡å¤„ç†ï¼‰ | LLM/Flow/HiFiGANç¼“å­˜è·¨chunkå…±äº« |
| **è¾¹ç•Œå¤„ç†** | ä¸é€‚ç”¨ï¼ˆå•æ¬¡å¤„ç†ï¼‰ | mel_cache_lenå’Œspeech_windowå¹³æ»‘ |
| **é€‚ç”¨åœºæ™¯** | çŸ­æ–‡æœ¬/ç‹¬ç«‹å¥å­<br>å¦‚ï¼š"ä»Šå¤©å¤©æ°”å¾ˆå¥½" | é•¿æ–‡æœ¬/è¿ç»­å¯¹è¯<br>å¦‚ï¼š["ä»Šå¤©", "å¤©æ°”", "å¾ˆå¥½"] |
| **å®æ—¶æµæ”¯æŒ** | éœ€è¦å®Œæ•´æ–‡æœ¬ | æ”¯æŒGeneratorå®æ—¶è¾“å…¥ |

**ä»£ç å¯¹æ¯”**:

```python
# ä¼ ç»Ÿæ–¹æ³• - ç‹¬ç«‹å¤„ç†
chunks = ["ä½ å¥½", "ä¸–ç•Œ"]
for chunk in chunks:
    # æ¯æ¬¡éƒ½æ˜¯æ–°UUIDï¼Œæ— ä¸Šä¸‹æ–‡è¿ç»­æ€§
    for result in model.inference_sft(chunk, spk_id="girl_zh"):
        audio = result['tts_speech']
        # éŸ³é¢‘ä¹‹é—´å¯èƒ½æœ‰ä¸è¿è´¯

# æ–°æ–¹æ³• - è¿è´¯å¤„ç†
chunks = ["ä½ å¥½", "ä¸–ç•Œ"]
# æ‰€æœ‰chunkå…±äº«ä¸€ä¸ªUUIDï¼Œå®Œå…¨è¿è´¯
for result in model.inference_sft_chunked(chunks, spk_id="girl_zh"):
    audio = result['tts_speech']
    # éŸ³é¢‘å®Œå…¨è¿è´¯ï¼Œå¦‚åŒä¸€æ¬¡æ€§è¾“å…¥"ä½ å¥½ä¸–ç•Œ"
```

---

## è¯·æ±‚åŒºåˆ†æœºåˆ¶

### å¦‚ä½•åŒºåˆ†åŒä¸€è¯·æ±‚çš„ä¸åŒchunk

**æ ¸å¿ƒç­”æ¡ˆ: ä¸éœ€è¦åŒºåˆ†ï¼**

è¿™æ˜¯æ•´ä¸ªè®¾è®¡æœ€å·§å¦™çš„åœ°æ–¹ â€”â€” ä»æ¨¡å‹çš„è§†è§’æ¥çœ‹ï¼Œ**æ ¹æœ¬ä¸å­˜åœ¨chunkçš„æ¦‚å¿µ**ã€‚

#### ç”¨æˆ·è§†è§’ vs æ¨¡å‹è§†è§’

```
ç”¨æˆ·è§†è§’:
text_chunks = ["ä½ å¥½", "ä¸–ç•Œ", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"]
             â†‘chunk1  â†‘chunk2  â†‘chunk3

æ¨¡å‹è§†è§’:
text_tokens = [ä½ , å¥½, ä¸–, ç•Œ, ä»Š, å¤©, å¤©, æ°”, å¾ˆ, å¥½]
              â† å®Œå…¨è¿ç»­çš„tokenæµï¼Œæ— è¾¹ç•Œæ ‡è®° â†’
```

#### å®ç°æœºåˆ¶

**åœ¨`inference_sft_chunked`ä¸­çš„å®ç°**:

```python
# cosyvoice.py: inference_sft_chunkedæ–¹æ³•

def text_token_generator():
    """
    å°†å¤šä¸ªæ–‡æœ¬chunkè½¬æ¢ä¸ºè¿ç»­çš„tokenæµ
    æ¨¡å‹å®Œå…¨æ„ŸçŸ¥ä¸åˆ°chunkè¾¹ç•Œ
    """
    for chunk in text_chunks:  # éå†æ‰€æœ‰chunk
        # 1. æ–‡æœ¬å½’ä¸€åŒ–ï¼ˆé€chunkï¼‰
        normalized_text = self.frontend.text_normalize(
            chunk,
            split=False
        )

        # 2. æå–tokenï¼ˆé€chunkï¼‰
        text_token, text_token_len = self.frontend._extract_text_token(
            normalized_text
        )

        # 3. é€token yieldï¼ˆæ— è¾¹ç•Œæ ‡è®°ï¼ï¼‰
        for i in range(text_token.shape[1]):
            yield text_token[:, i:i+1]
            # â†‘ å…³é”®ï¼šä¸€ä¸ªtokenä¸€ä¸ªtokenåœ°yield
            # æ¨¡å‹çœ‹åˆ°çš„å°±æ˜¯è¿ç»­çš„tokenæµ
```

**ä¸ºä»€ä¹ˆä¸éœ€è¦è¾¹ç•Œæ ‡è®°ï¼Ÿ**

1. **Generatorçš„è¿ç»­æ€§**:
   - Generatoré€ä¸ªyield tokenï¼Œchunkä¹‹é—´æ— åœé¡¿ã€æ— æ ‡è®°
   - LLMçš„`inference_bistream`åªæ˜¯ä¸æ–­æ¶ˆè´¹ä¸‹ä¸€ä¸ªtoken

2. **KV-cacheçš„è¿ç»­æ€§**:
   - Transformerçš„KV-cacheè·¨æ‰€æœ‰tokenæŒç»­ç´¯ç§¯
   - ç¬¬2ä¸ªchunkçš„ç¬¬1ä¸ªtokenï¼Œä½¿ç”¨çš„ä¸Šä¸‹æ–‡åŒ…å«ç¬¬1ä¸ªchunkçš„æ‰€æœ‰token
   - å°±åƒä¸€æ¬¡æ€§è¾“å…¥å®Œæ•´æ–‡æœ¬ä¸€æ ·

3. **æ–‡æœ¬å½’ä¸€åŒ–çš„ç‹¬ç«‹æ€§**:
   - è™½ç„¶æ¯ä¸ªchunkç‹¬ç«‹å½’ä¸€åŒ–ï¼Œä½†è¿™ä¸å½±å“è¯­ä¹‰
   - å½’ä¸€åŒ–åªæ˜¯æŠŠ"100"è½¬æˆ"ä¸€ç™¾"ï¼Œä¸å½±å“ä¸Šä¸‹æ–‡ç†è§£

**è¿è´¯æ€§ä¿è¯**:

```python
# LLMå†…éƒ¨çš„å¤„ç†ï¼ˆç®€åŒ–ç‰ˆï¼‰
def inference_bistream(self, text: Generator, ...):
    kv_cache = []  # åˆå§‹åŒ–ç©ºç¼“å­˜

    for text_token in text:  # æ¶ˆè´¹Generator
        # ä½¿ç”¨ç´¯ç§¯çš„KV-cacheè¿›è¡Œattention
        # chunk1çš„tokenä¼šè¿›å…¥cache
        # chunk2çš„tokenä½¿ç”¨chunk1å»ºç«‹çš„cache
        output, kv_cache = transformer(
            text_token,
            kv_cache=kv_cache  # â† æŒç»­ç´¯ç§¯çš„ä¸Šä¸‹æ–‡
        )
        yield output
```

---

### å¦‚ä½•åŒºåˆ†ä¸åŒçš„è¯·æ±‚

**å…³é”®æœºåˆ¶: UUIDä¼šè¯ç®¡ç†**

æ¯æ¬¡è°ƒç”¨`inference_sft_chunked`æ—¶ï¼Œéƒ½ä¼šç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„UUIDï¼Œä»è€Œåˆ›å»ºå®Œå…¨ç‹¬ç«‹çš„ç¼“å­˜ç©ºé—´ã€‚

#### UUIDç”Ÿæˆæ—¶æœº

```python
# cosyvoice.py: inference_sft_chunked
def inference_sft_chunked(self, text_chunks, spk_id, ...):
    # å‡†å¤‡è¾“å…¥
    model_input = {
        'text': text_token_generator(),  # Generator
        'llm_embedding': embedding,
        'flow_embedding': embedding,
        # æ³¨æ„ï¼šè¿™é‡Œä¸ä¼ uuidï¼Œè®©model.ttsè‡ªå·±ç”Ÿæˆ
    }

    # è°ƒç”¨åº•å±‚ttsæ–¹æ³•
    for model_output in self.model.tts(**model_input, stream=stream):
        # â†‘ åœ¨tts()æ–¹æ³•å†…éƒ¨ç”Ÿæˆæ–°çš„UUID
        yield model_output
```

```python
# model.py: ttsæ–¹æ³•
def tts(self, text, llm_embedding, flow_embedding, ...):
    # æ¯æ¬¡è°ƒç”¨ç”Ÿæˆå…¨æ–°UUID
    uuid = str(uuid4())
    # â†‘ è¿™å°±æ˜¯è¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ç¬¦

    # ä¸ºè¿™ä¸ªUUIDåˆ›å»ºç‹¬ç«‹çš„ç¼“å­˜ç©ºé—´
    self.llm_cache[uuid] = {}
    self.flow_cache[uuid] = {}
    self.hift_cache[uuid] = {}

    try:
        # æ‰§è¡Œæ¨ç†ï¼Œä½¿ç”¨uuidå¯¹åº”çš„ç¼“å­˜
        for output in self._do_inference(uuid, ...):
            yield output
    finally:
        # æ¨ç†å®Œæˆï¼Œæ¸…ç†è¿™ä¸ªUUIDçš„ç¼“å­˜
        del self.llm_cache[uuid]
        del self.flow_cache[uuid]
        del self.hift_cache[uuid]
```

#### è¯·æ±‚éš”ç¦»ç¤ºä¾‹

```python
# åœºæ™¯ï¼šä¸¤ä¸ªç‹¬ç«‹çš„æ¨ç†è¯·æ±‚

# ========== è¯·æ±‚1 ==========
for audio in model.inference_sft_chunked(
    text_chunks=["ä½ å¥½", "ä¸–ç•Œ"],
    spk_id="girl_zh"
):
    play(audio)

# å†…éƒ¨æµç¨‹:
# 1. ç”Ÿæˆ UUID-AAAA
# 2. åˆ›å»º llm_cache[UUID-AAAA] = {}
# 3. å¤„ç† "ä½ å¥½" + "ä¸–ç•Œ"
# 4. yield éŸ³é¢‘chunks
# 5. æ¸…ç† llm_cache[UUID-AAAA]

# ========== è¯·æ±‚2 (å®Œå…¨ç‹¬ç«‹) ==========
for audio in model.inference_sft_chunked(
    text_chunks=["å†è§", "æœ‹å‹"],
    spk_id="girl_zh"
):
    play(audio)

# å†…éƒ¨æµç¨‹:
# 1. ç”Ÿæˆ UUID-BBBB (å…¨æ–°çš„UUID!)
# 2. åˆ›å»º llm_cache[UUID-BBBB] = {}  â† ä¸UUID-AAAAå®Œå…¨éš”ç¦»
# 3. å¤„ç† "å†è§" + "æœ‹å‹"
# 4. yield éŸ³é¢‘chunks
# 5. æ¸…ç† llm_cache[UUID-BBBB]

# ä¸¤ä¸ªè¯·æ±‚äº’ä¸å½±å“ï¼š
# - UUID-AAAAçš„KV-cacheä¸ä¼šå½±å“UUID-BBBB
# - UUID-BBBBçš„melç¼“å­˜ä¸ä¼šå½±å“UUID-AAAA
```

#### ç¼“å­˜ç»“æ„

```python
# æ¨¡å‹å†…éƒ¨çš„ç¼“å­˜å­—å…¸ç»“æ„
{
    'llm_cache': {
        'uuid-1234-...': {
            'k_cache': [...],  # Transformer Key cache
            'v_cache': [...],  # Transformer Value cache
        },
        'uuid-5678-...': {
            'k_cache': [...],  # å®Œå…¨ç‹¬ç«‹çš„ç¼“å­˜
            'v_cache': [...],
        }
    },

    'flow_cache': {
        'uuid-1234-...': {
            'last_mel_frames': [...],  # ä¸Šæ¬¡çš„melè¾¹ç•Œå¸§
        },
        'uuid-5678-...': {
            'last_mel_frames': [...],  # ç‹¬ç«‹çš„melç¼“å­˜
        }
    },

    'hift_cache': {
        'uuid-1234-...': {
            'last_audio_samples': [...],  # ä¸Šæ¬¡çš„éŸ³é¢‘è¾¹ç•Œ
        },
        'uuid-5678-...': {
            'last_audio_samples': [...],  # ç‹¬ç«‹çš„éŸ³é¢‘ç¼“å­˜
        }
    }
}
```

---

### ç¼“å­˜ç”Ÿå‘½å‘¨æœŸ

#### å®Œæ•´ç”Ÿå‘½å‘¨æœŸå›¾

```
æ—¶é—´çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

è°ƒç”¨å¼€å§‹                                              è°ƒç”¨ç»“æŸ
   â†“                                                    â†“
[ç”ŸæˆUUID]â”€â†’[åˆ›å»ºç¼“å­˜]â”€â†’[å¤„ç†chunk1]â”€â†’[å¤„ç†chunk2]â”€â†’[æ¸…ç†ç¼“å­˜]
 uuid-A       cache[A]={}   ä½¿ç”¨cache[A]  ä½¿ç”¨cache[A]  del cache[A]
                            â†“              â†“
                         ç´¯ç§¯KV-cache   ç»§ç»­ç´¯ç§¯KV-cache
                         å­˜å‚¨melè¾¹ç•Œ    ä½¿ç”¨+æ›´æ–°melè¾¹ç•Œ
                         å­˜å‚¨éŸ³é¢‘è¾¹ç•Œ   ä½¿ç”¨+æ›´æ–°éŸ³é¢‘è¾¹ç•Œ

                                                        ç¼“å­˜é‡Šæ”¾
                                                           â†“
                                                        å†…å­˜å›æ”¶

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ä¸‹ä¸€æ¬¡è°ƒç”¨å¼€å§‹
   â†“
[ç”ŸæˆUUID]â”€â†’[åˆ›å»ºç¼“å­˜]â”€â†’ ...
 uuid-B       cache[B]={}  â† å…¨æ–°çš„ç¼“å­˜ç©ºé—´ï¼Œä¸å—uuid-Aå½±å“
              (ç©ºç™½çŠ¶æ€)
```

#### è¯¦ç»†æ­¥éª¤

**ç¬¬1é˜¶æ®µ: åˆå§‹åŒ– (è°ƒç”¨å¼€å§‹æ—¶)**

```python
# ç”¨æˆ·è°ƒç”¨
for audio in model.inference_sft_chunked(["ä½ å¥½", "ä¸–ç•Œ"], ...):

# å†…éƒ¨æ‰§è¡Œ
uuid = str(uuid4())  # å¦‚: "a1b2c3d4-5678-..."

# åˆ›å»ºç©ºç¼“å­˜
self.llm_cache[uuid] = {}
self.flow_cache[uuid] = {}
self.hift_cache[uuid] = {}
```

**ç¬¬2é˜¶æ®µ: ç´¯ç§¯ (å¤„ç†chunkè¿‡ç¨‹ä¸­)**

```python
# å¤„ç†ç¬¬1ä¸ªchunk "ä½ å¥½"
# - llm_cache[uuid] ç´¯ç§¯ "ä½ ", "å¥½" çš„KV-cache
# - flow_cache[uuid] å­˜å‚¨æœ€å8å¸§mel
# - hift_cache[uuid] å­˜å‚¨æœ€å3840ä¸ªéŸ³é¢‘æ ·æœ¬

# å¤„ç†ç¬¬2ä¸ªchunk "ä¸–ç•Œ"
# - llm_cache[uuid] ç»§ç»­ç´¯ç§¯ "ä¸–", "ç•Œ" çš„KV-cache
#   (æ­¤æ—¶åŒ…å« "ä½ å¥½ä¸–ç•Œ" çš„å®Œæ•´ä¸Šä¸‹æ–‡)
# - flow_cache[uuid] ä½¿ç”¨ä¹‹å‰çš„melè¾¹ç•Œï¼Œç”Ÿæˆå¹³æ»‘è¿‡æ¸¡
# - hift_cache[uuid] ä½¿ç”¨ä¹‹å‰çš„éŸ³é¢‘è¾¹ç•Œï¼Œæ·¡å…¥æ·¡å‡º
```

**ç¬¬3é˜¶æ®µ: æ¸…ç† (è°ƒç”¨ç»“æŸæ—¶)**

```python
# Generatorè€—å°½ï¼ˆæ‰€æœ‰éŸ³é¢‘å·²yieldï¼‰
finally:
    # æ¸…ç†ç¼“å­˜
    del self.llm_cache[uuid]
    del self.flow_cache[uuid]
    del self.hift_cache[uuid]

    # Python GCä¼šè‡ªåŠ¨å›æ”¶å†…å­˜
```

---

### å¹¶å‘è¯·æ±‚æ”¯æŒ

ç”±äºUUIDæœºåˆ¶ï¼Œ`inference_sft_chunked` **å¤©ç„¶æ”¯æŒå¹¶å‘è¯·æ±‚**ï¼Œæ— éœ€é¢å¤–çš„é”æˆ–åŒæ­¥æœºåˆ¶ã€‚

#### å¹¶å‘å®‰å…¨æ€§

```python
# å‡è®¾æœ‰3ä¸ªå¹¶å‘è¯·æ±‚
# è¯·æ±‚A: UUID-AAAA, å¤„ç† "ä½ å¥½ä¸–ç•Œ"
# è¯·æ±‚B: UUID-BBBB, å¤„ç† "æ—©ä¸Šå¥½å•Š"
# è¯·æ±‚C: UUID-CCCC, å¤„ç† "æ™šå®‰æœ‹å‹"

# ç¼“å­˜çŠ¶æ€:
{
    'llm_cache': {
        'UUID-AAAA': {KV for "ä½ å¥½ä¸–ç•Œ"},
        'UUID-BBBB': {KV for "æ—©ä¸Šå¥½å•Š"},
        'UUID-CCCC': {KV for "æ™šå®‰æœ‹å‹"}
    }
}

# ä¸‰ä¸ªè¯·æ±‚å®Œå…¨éš”ç¦»ï¼Œäº’ä¸å½±å“ï¼
```

#### å¼‚æ­¥å¹¶å‘ç¤ºä¾‹

```python
import asyncio
from cosyvoice.cli.cosyvoice import CosyVoice2

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¨å±€å…±äº«ï¼‰
model = CosyVoice2('/path/to/model')

async def process_request(text_chunks, request_id):
    """
    å¤„ç†å•ä¸ªTTSè¯·æ±‚

    æ¯ä¸ªè¯·æ±‚ä¼šè‡ªåŠ¨è·å¾—ç‹¬ç«‹çš„UUIDå’Œç¼“å­˜ç©ºé—´
    """
    print(f"Request {request_id} started")

    audio_segments = []
    for result in model.inference_sft_chunked(
        text_chunks=text_chunks,
        spk_id="girl_zh",
        stream=True,
        token_hop_len=15
    ):
        audio = result['tts_speech']
        audio_segments.append(audio)

        # æ¨¡æ‹Ÿå®æ—¶å‘é€éŸ³é¢‘
        await send_to_client(audio, request_id)

    print(f"Request {request_id} completed")
    return audio_segments

# å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
async def main():
    # åŒæ—¶å¤„ç†3ä¸ªè¯·æ±‚
    results = await asyncio.gather(
        process_request(["ä½ å¥½", "ä¸–ç•Œ"], request_id=1),      # UUID-AAA
        process_request(["æ—©ä¸Š", "å¥½å•Š"], request_id=2),      # UUID-BBB
        process_request(["æ™šå®‰", "æœ‹å‹", "æ˜å¤©è§"], request_id=3)  # UUID-CCC
    )

    print("All requests completed!")

# è¿è¡Œ
asyncio.run(main())
```

**è¾“å‡ºç¤ºä¾‹**:
```
Request 1 started
Request 2 started
Request 3 started
Request 2 completed  (æœ€å¿«å®Œæˆï¼Œåªæœ‰2ä¸ªchunk)
Request 1 completed
Request 3 completed  (æœ€æ…¢å®Œæˆï¼Œæœ‰3ä¸ªchunk)
All requests completed!
```

#### å¹¶å‘æ€§èƒ½è€ƒè™‘

**ä¼˜åŠ¿**:
- âœ… **æ— é”è®¾è®¡**: UUIDæœºåˆ¶å¤©ç„¶æ”¯æŒå¹¶å‘ï¼Œæ— éœ€é”
- âœ… **ç‹¬ç«‹ç¼“å­˜**: æ¯ä¸ªè¯·æ±‚æœ‰ç‹¬ç«‹å†…å­˜ç©ºé—´ï¼Œæ— ç«äº‰
- âœ… **å¼¹æ€§æ‰©å±•**: å¯ä»¥åŒæ—¶å¤„ç†ä»»æ„æ•°é‡çš„è¯·æ±‚

**æ³¨æ„äº‹é¡¹**:
- âš ï¸ **GPUæ˜¾å­˜**: å¹¶å‘è¯·æ±‚å…±äº«GPUï¼Œæ˜¾å­˜æ˜¯ç“¶é¢ˆ
  - å»ºè®®: æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…OOM
  - æˆ–è€…: ä½¿ç”¨é˜Ÿåˆ—æ’é˜Ÿå¤„ç†

- âš ï¸ **CPUç¼“å­˜**: æ¯ä¸ªè¯·æ±‚çš„KV-cacheä¼šå ç”¨å†…å­˜
  - å»ºè®®: ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ŒåŠæ—¶æ¸…ç†å®Œæˆçš„è¯·æ±‚

**æ¨èå¹¶å‘ç­–ç•¥**:

```python
# ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
semaphore = asyncio.Semaphore(3)  # æœ€å¤š3ä¸ªå¹¶å‘è¯·æ±‚

async def process_with_limit(text_chunks, request_id):
    async with semaphore:  # è·å–ä¿¡å·é‡
        return await process_request(text_chunks, request_id)

# å³ä½¿æœ‰10ä¸ªè¯·æ±‚ï¼ŒåŒæ—¶åªä¼šå¤„ç†3ä¸ª
tasks = [
    process_with_limit(chunks, i)
    for i, chunks in enumerate(all_requests)
]
results = await asyncio.gather(*tasks)
```

---

## æ€»ç»“

### å…³é”®è®¾è®¡äº®ç‚¹

1. **é›¶ä¿®æ”¹æ ¸å¿ƒæ¨¡å‹**
   - å®Œå…¨åŸºäºç°æœ‰åŠŸèƒ½å®ç°
   - åˆ©ç”¨LLMçš„`inference_bistream`æ¥å—Generatorè¾“å…¥
   - åˆ©ç”¨UUIDä¼šè¯æœºåˆ¶ç®¡ç†çŠ¶æ€

2. **ä¼˜é›…çš„chunkå¤„ç†**
   - ç”¨æˆ·å±‚é¢: æ˜ç¡®çš„chunkåˆ’åˆ†
   - æ¨¡å‹å±‚é¢: å®Œå…¨è¿ç»­çš„tokenæµ
   - æ— éœ€è¾¹ç•Œæ ‡è®°ï¼Œè‡ªç„¶ä¿æŒè¿è´¯

3. **å¼ºå¤§çš„è¯·æ±‚éš”ç¦»**
   - æ¯ä¸ªè¯·æ±‚ä¸€ä¸ªUUID
   - ç‹¬ç«‹çš„ç¼“å­˜ç©ºé—´
   - å¤©ç„¶æ”¯æŒå¹¶å‘

4. **çµæ´»çš„å‚æ•°æ§åˆ¶**
   - `token_hop_len`: æ§åˆ¶è¾“å‡ºchunkå¤§å°
   - `mel_cache_len`: æ§åˆ¶å¹³æ»‘åº¦
   - å¯æ ¹æ®åœºæ™¯è°ƒä¼˜ï¼ˆå»¶è¿Ÿ vs è´¨é‡ï¼‰

### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ¨èæ–¹æ³• | åŸå›  |
|------|---------|------|
| çŸ­æ–‡æœ¬TTS | `inference_sft` | ç®€å•ç›´æ¥ï¼Œæ— éœ€chunk |
| é•¿æ–‡æœ¬TTS | `inference_sft_chunked` | ä¿æŒè¿è´¯ï¼Œæ”¯æŒæµå¼ |
| å®æ—¶å¯¹è¯ | `inference_sft_chunked` + Generator | å®æ—¶è¾“å…¥ï¼Œä½å»¶è¿Ÿ |
| æ‰¹é‡å¤„ç† | `inference_sft_chunked` | é«˜æ•ˆï¼Œå¯è°ƒä¼˜ |
| å¹¶å‘æœåŠ¡ | `inference_sft_chunked` | å¤©ç„¶æ”¯æŒå¹¶å‘ |

### å‚è€ƒæ–‡ä»¶

**æ ¸å¿ƒå®ç°**:
- `cosyvoice/cli/cosyvoice.py:202-280` - `inference_sft_chunked`æ–¹æ³•
- `cosyvoice/llm/llm.py:505-602` - `inference_bistream`æ–¹æ³•
- `cosyvoice/cli/model.py` - UUIDä¼šè¯ç®¡ç†

**ç›¸å…³æœºåˆ¶**:
- `cosyvoice/flow/flow.py:161` - Flow streamingæ”¯æŒ
- `cosyvoice/cli/model.py:272-277` - å¹³æ»‘å‚æ•°é…ç½®

**ç¤ºä¾‹ä»£ç **:
- `run_chunked.py` - æ–°æ—§æ–¹æ³•å¯¹æ¯”ç¤ºä¾‹

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11
